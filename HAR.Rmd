---
title: "Course Project"
author: "Bruno Pimentel"
date: "May 18, 2020"
output: html_document
---


### Introduction

This document is part of course project about **Practical Machine Learning** from Cousera. This project concerns about prediction of personal activity. Devices such as Jawbone Up, Nike FuelBand, and Fitbit were used to collect quantified self movement. The goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants and construct models to predict 20 different test cases. The rest of document discuss about the steps used to obtain the results: pre-processing, evaluation and error analysis.

### Pre-processing

In this step, the data in .csv file is loaded into memory. After, in order to avoid errors in the evaluation step, some unnecessary variables are removed from the original data. After this pre-processing step, 18 variables are considered in the final data. The following code shows the library used and the function to pre-process the data.

```{r message=FALSE, warning=FALSE}
library(randomForest)
library(rpart)
library(rpart.plot)
require(splines)
```

```{r}
#-------- Pre-processing

#Load data and remove variables
pre_process <- function(name_file){
  data <- read.csv(file = paste0('Data/',name_file,'.csv'))
  data <- data[,-append(append(1:7, 12:36), 50:159)]
  data[is.na(data)] <- 0
  return(data)
}

data_training <- pre_process('pml-training')
data_testing <- pre_process('pml-testing')

```

The table containing the selected variables are shown as following:

```{r}
head(data_training)
```

### Evaluation

The goal of this step is to evaluate the models used in this project. For this, a function that compute the error between the prediction of model and the original target was used. In order to evaluate the models, the k-fold cross validation method was adopted for training data, were k=10. In each iteration of cross validation, a fold is selected as testing set and the remaining 9 folds are selected as training set. For each testing set, the prediction error is stored, thus after executing the 10 folds, 10 error values are computed and, then, the mean and standard deviation of these error values are calculated. In this project, two models were considered: Random Forest (here abbreviated as RF) and Decision Tree (here abbreviated as DT). For Random Forest, 200 trees were used. The following code shows the error of prediction for each model at each iteration of cross validation method.

```{r}
#-------- Cross validation

#Compute error
compute_error <- function(y_pred, y_test){
  error <- 1.0 - sum(y_test == y_pred)/length(y_test)
  return(error)
}

set.seed(1000)
n_train <- nrow(data_training)
n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = n_train))
cv_tmp <- matrix(NA, n_folds, 2)
for (k in 1:n_folds) {
  print(k)
  test_i <- which(folds_i == k)
  
  #Training set
  train_xy <- data_training[-test_i, ]
  x_train <- subset(train_xy, select = -classe)
  y_train <- train_xy$classe
  
  #Testing set
  test_xy <- data_training[test_i, ]
  x_test <- subset(test_xy, select = -classe)
  y_test <- test_xy$classe
  
  #Random Forest (RF)
  fitted_RF <- randomForest(x_train, y_train, prox=TRUE, ntree=200)
  #Decision Tree (DT)
  fitted_DT <- rpart(formula = classe ~ ., data = train_xy,control = rpart.control(minsplit = 10, maxdepth = 15))
  
  #Predictions on testing set
  pred_RF <- predict(fitted_RF,x_test) 
  error_RF <- compute_error(pred_RF, y_test)
  print(error_RF)
  
  pred_DT <- predict(fitted_DT, newdata = x_test, type = "class") 
  error_DT <- compute_error(pred_DT, y_test)
  print(error_DT)
  
  cv_tmp[k,1] <- error_RF
  cv_tmp[k,2] <- error_DT
}
```

### Cross Validation Analysis

The folowing function prints the mean and standard deviation of error prediction values after executing cross validation method:

```{r}
#Print mean and standard deviation
print_result <- function(name, result){
  cat('\n')
  cat(name)
  cat(': Mean = ')
  cat(mean(result))
  cat(' Std = ')
  cat(sd(result))
}

#-------- Results
cat('\n')
cat("Results:")
print_result('RF', cv_tmp[,1])
print_result('DT', cv_tmp[,2])
```

As expected, Random Forest obtained better result than Decision Tree, since Random Forest used 200 trees to make the prediction. 

After executing the cross validation method, the models used in the experiment can show important information about the prediction. The following figure shows the rules obtained by the fitted Decision Tree:

```{r}
#Tree
rpart.plot(fitted_DT)
```

In order to analyse the relevance variables for prediction of the Random Forest model, following figure shows the importance of variables that descrive the data.

```{r}
#-------- Figures
#Importances
importances <- importance(fitted_RF)
variables_name = row.names(importances)
barplot(as.vector(importances), main="Importance of variables",xlab=NULL, ylab = "Importance", horiz=FALSE, names.arg=variables_name, las=2, cex.names = 0.7)
```

From this figure, it is possible to highlight that "roll_belt", "pitch_belt" and "yaw_belt" are the most relevante for prediction of Random Forest model. The following figure shows the Out of Bag (OOB) score in Random Forest model according to variation of number of trees.

```{r}
#Number of trees versus error
layout(matrix(c(1,2),nrow=1), width=c(4,1))
plot(fitted_RF, main="Number of trees versus error", col=10:15)
legend("top", inset=c(0,0), colnames(fitted_RF$err.rate),col=10:15,cex=0.5,fill=10:15)
```

As expected, for small number of trees, the error is higher. However, the error decreases as the number of trees increases. Also, it is possible to conclude that the error stabilizes when the number of trees is arround 50. It is also important to highlight that the error for class "E" is higher than error for class "C", whatever the number of trees used in the Random Forest model. The following figure shows the dispersion of measures from training data regarding variables "roll_belt", "pitch_belt" and "yaw_belt".

```{r}
#Most important variables
plot(data_training[,1:3], col=data_training$classe)
```

### Test Data Prediction

The prediction of test data is given as following:

```{r}
#Prediction of test data
pred_RF <- predict(fitted_RF,data_testing) 
cat('\n Test prediction (RF): ', as.vector(pred_RF))

pred_DT <- predict(fitted_DT, newdata = data_testing, type = "class") 
cat('\n Test prediction (DT): ', as.vector(pred_DT))
```

The results point out that Random Forest and Decision Tree models obtained a similar prediction. The difference is in the 5th element, where the prediction of RF was "A" and the prediction of DT was "B", and in the 19th element, where the prediction of RF was "B" and the prediction of DT was "A".
