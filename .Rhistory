library(readr)
arle_completo_agregado_2020_02 <- read_csv("GitHub/Tarefas/Dados/arle_completo_agregado_2020_02.csv")
View(arle_completo_agregado_2020_02)
View(arle_completo_agregado_2020_02)
View(arle_completo_agregado_2020_02)
v <- c("IDUnique", "IDALUNO", "Tipo_Tentativa", "RESULTADO", "LATENCIA", "Tam_Modelo", "M_C1", "M_C2", "M_C3", "C1_C2", "C1_C3", "C2_C3", "M_Comps_Sum")
data = arle_completo_agregado_2020_02[v]
View(data)
View(data)
write.csv(data, "arle_completo_dist.csv")
library(readr)
arle_completo_dist <- read_csv("GitHub/Tarefas/Dados/arle_completo_dist.csv")
View(arle_completo_dist)
install.packages("stringdist")
# Simple example using optimal string alignment
stringdist("ca","abc")
# computing a 'dist' object
d <- stringdistmatrix(c('foo','bar','boo','baz'))
stringdist(a, b, method = c("osa", "lv", "dl", "hamming", "lcs", "qgram",
"cosine", "jaccard", "jw", "soundex"), useBytes = FALSE, weight = c(d
= 1, i = 1, s = 1, t = 1), q = 1, p = 0, bt = 0,
nthread = getOption("sd_num_thread"))
stringdistmatrix(a, b, method = c("osa", "lv", "dl", "hamming", "lcs",
"qgram", "cosine", "jaccard", "jw", "soundex"), useBytes = FALSE,
weight = c(d = 1, i = 1, s = 1, t = 1), q = 1, p = 0, bt = 0,
useNames = c("none", "strings", "names"),
nthread = getOption("sd_num_thread"))
# Simple example using optimal string alignment
stringdist("ca","abc")
d <- stringdistmatrix(c('foo','bar','boo','baz'))
source(stringdist)
source("stringdist")
library("stringdist")
d <- stringdistmatrix(c('foo','bar','boo','baz'))
d
stringdistmatrix(c("foo","bar","boo"),c("baz","buz"))
stringdist("ab","abc",method="h")
stringdist("hello","HeLl0",method="h")
stringdist("hello","HeLl0",method="cosine")
stringdist("hello","Hello",method="cosine")
stringdist("hello","hello",method="cosine")
stringdist("hello","hello ",method="cosine")
stringdist("hello","hello",method="cosine")
a <- " xx yy 11 22 33 "
str_replace_all(string=a, pattern=" ", repl="")
a=0
a+4
e
exp(1)
exp(-1)
a=0
a[1] = 1
a
a[2] = 3
a
runif(4,0,1)
a=runif(4,0,1)
a[1]
source("C:/Users/Bruno Pimentel/Desktop/desvios.R")
desvio()
round(0.03124423455, digits=2)
source("C:/Users/Bruno Pimentel/Desktop/desvios.R")
desvio()
source("C:/Users/Bruno Pimentel/Desktop/desvios.R")
desvio()
source("C:/Users/Bruno Pimentel/Desktop/desvios.R")
desvio()
desvio()
desvio()
source("C:/Users/Bruno Pimentel/Desktop/desvios.R")
desvio()
desvio()
desvio()
desvio()
desvio()
desvio()
desvio()
desvio()
desvio()
desvio()
desvio()
source("C:/Users/Bruno Pimentel/Desktop/desvios.R")
desvio()
aes()
check()
sessionInfo()
install.packages("KernSmooth)
""
)0
;
00pok
''
''"
install.packages("KernSmooth")
packageDescription('KernSmooth')[]
library(KernSmooth)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
source("a")
data(vowel.test)
source('C:/Users/Bruno Pimentel/Desktop/a.R')
install.packages("ElemStatLearn")
library(ElemStatLearn)
create = createSynthetic(5);
source("loadDataset.r");
setwd("C:/Users/Bruno Pimentel/Documents/GitHub/SDA");
create = createSynthetic(indexData);
source("loadDataset.r");
create = createSynthetic(indexData);
create = createSynthetic(5);
data = create$syn;
ref = create$ref;
nClusters = create$nClusters;
nObj = nrow(data);
folds_i <- sample(rep(1:10, length.out = nObj))
test_i <- which(folds_i == k)
test_i <- which(folds_i == 1)
x_train <- data[-test_i, ]
y_train <- ref[-test_i, ]
y_train <- ref[-test_i]
y_train
test_i
count(y_test)
plot(y_test)
plot(y_train)
y_train==1
sum(y_train==1)
data_train = cbind(x_train, y_train)
View(data_train)
View(data_train)
x_train <- data[-test_i, c(1,2)]
y_train <- data[-test_i, c(3,4)]
data_train = cbind(x_train, y_train)
View(data_train)
View(data_train)
setwd("C:/Users/Bruno Pimentel/Documents/GitHub/SDA");source("experiment_cv.r");
data(cars)
reg <- lm(log(dist) ~ log(speed), data = cars)
MAPE(y_pred = exp(reg$fitted.values), y_true = cars$dist)
install.packages("MLmetrics")
library(MLmetrics);
MAPE(y_pred = exp(reg$fitted.values), y_true = cars$dist)
MAPE(y_pred = exp(reg$fitted.values), y_true = cars$dist)
error = MAPE(y_pred = exp(reg$fitted.values), y_true = cars$dist)
source('~/GitHub/datasciencecoursera/main.R')
source('~/GitHub/datasciencecoursera/main.R')
source('~/GitHub/datasciencecoursera/main.R')
```{r, chunk-label}
library(randomForest)
library(rpart)
library(rpart.plot)
require(splines)
```
library(randomForest)
library(rpart)
library(rpart.plot)
require(splines)
#-------- Pre-processing
#Load data and remove variables
pre_process <- function(name_file){
data <- read.csv(file = paste0('Data/',name_file,'.csv'))
data <- data[,-append(append(1:7, 12:36), 50:159)]
data[is.na(data)] <- 0
return(data)
}
data_training <- pre_process('pml-training')
data_testing <- pre_process('pml-testing')
knitr::opts_chunk$set(echo = TRUE)
#-------- Figures
#Importances
importances <- importance(fitted_RF)
variables_name = row.names(importances)
barplot(as.vector(importances), main="Importance of variables",xlab=NULL, ylab = "Importance", horiz=FALSE, names.arg=variables_name, las=2, cex.names = 0.7)
#Tree
rpart.plot(fitted_DT)
#Number of trees versus error
layout(matrix(c(1,2),nrow=1), width=c(4,1))
plot(fitted_RF, main="Number of trees versus error", col=10:15)
legend("top", inset=c(0,0), colnames(fitted_RF$err.rate),col=10:15,cex=0.5,fill=10:15)
#Most important variables
plot(data_training[,1:3], col=data_training$classe)
#Pair of variables
a = 10
b = 5
plot(x=data_training[,a], y=data_training[,b], xlab=variables_name[a], ylab=variables_name[b], col=data_training$classe)
